from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
from airflow.hooks.base_hook import BaseHook
from airflow.hooks.http_hook import HttpHook
from airflow.hooks.webhdfs_hook import WebHDFSHook
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import ShortCircuitOperator
from airflow.configuration import conf as airflow_conf
from datetime import timedelta
from pipeline.operators.move_processed_incr_partitions_operator import MoveProcessedIncrPartitionsOperator
from pipeline.operators.update_hive_partition_operator import UpdateHivePartitionOperator
import glob
import json
import os
import configparser


def getPipelineDefaultArgs():
    return {
        'owner': 'SuperDB team',
        'email': ['dev-spdb-pipeline-notifications-prod@mail.rakuten.com'],
        'email_on_failure': True,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=1),
    }


def getGlobalConfig():
    with open(getDagsBasePath() + "/pipeline/config/_global.json") as json_data:
        return json.load(json_data)


def get_data_source_qns():
    return [d for d in os.listdir(getDagsBasePath() + "/pipeline/config") if not d.startswith('_')]


def get_bool_str_from_dict(src_dict, key):
    return 'true' if src_dict.get(key) else ''


def getTableConfigs(dataSourceQn):
    tableConfigs = []
    for tableConfigFile in glob.glob(getDagsBasePath() + "/pipeline/config/{0}/*.json".format(dataSourceQn)):
        with open(tableConfigFile) as json_data:
            try:
                tableConfigs.append(json.load(json_data))
            except Exception as e:
                raise Exception(f"Failed to read {tableConfigFile}. {str(e)}")
    return tableConfigs


def incr_files_exist(path, partition_key):
    hdfs_conn = WebHDFSHook(webhdfs_conn_id="webhdfs_default").get_conn()
    return hdfs_conn.status(path, strict=False) is not None \
        and any(partition_dir.startswith(partition_key + '=') for partition_dir in hdfs_conn.list(path))


def getCheckIncrPartitionsTask(tableConfig, databaseConfig, dag):
    return ShortCircuitOperator(
        task_id=tableConfig['name'] + '__CheckIncrPartitions',
        python_callable=incr_files_exist,
        op_kwargs={
            'path': '{db}/_incr_files/{topic}'.format(db=databaseConfig['hiveDatabase'], topic=tableConfig['kafkaTopic']),
            'partition_key': 'partition'
        },
        dag=dag)


def getMoveIncrPartitionsTask(tableConfig, databaseConfig, dag):
    return MoveProcessedIncrPartitionsOperator(
        task_id=tableConfig['name'] + "__MoveIncrPartitions",
        origin_path='{database}/_incr_files/{topic}'.format(
            database=databaseConfig['hiveDatabase'], topic=tableConfig['kafkaTopic']),
        destination_path='{database}/_incr_files_hist/{topic}'.format(
            database=databaseConfig['hiveDatabase'], topic=tableConfig['kafkaTopic']),
        partition_key='partition',
        last_partition='{{{{ dag_run.get_task_instance("{sparkTask}").start_date.strftime("%Y-%m-%d-%H") }}}}'.format(
            sparkTask=tableConfig['name'] + '__UpdateQueryTableData'),
        skip_flag_file='_SCHEMA_CHANGE_COLUMN_ADDED',
        # In case of Oracle Hist solution, partitions already moved might get regenerated by Oracle OracleHistMergeTask
        # So for Oracle, if the partition about to be moved is present in hist too, need to delete that and then move
        webhdfs_conn_id='webhdfs_default',
        dag=dag)


# Old metadata task that uses schema-utils
def getUpdateQueryTableMetadataTaskOld(tableConfig, databaseConfig, dag):
    return BashOperator(
        task_id=tableConfig['name'] + "__UpdateQueryTableMetadata",
        bash_command="""mkdir -p {jarsPath};
                        if [ ! -f {jarsPath}/schema-util-1.0.jar ]; then
                            echo "Downloading the jar file";
                            wget -P {jarsPath} https://artifactory.rakuten-it.com/spdb-mvn/com/rakuten/dpd/pipeline/schema/schema-util/1.0/schema-util-1.0.jar;
                        fi
                        java -jar {jarsPath}/schema-util-1.0.jar -cluster c5000 -env prod -datasource_qn {dataSourceQn} \
                        -partition_cols [{queryTablePartitionColumn}] -dataset_id {dataSetId}""".format(
            jarsPath='/usr/local/airflow/jars',
            dataSourceQn=databaseConfig['dataSourceQn'],
            queryTablePartitionColumn=tableConfig['queryTablePartitionColumn'],
            dataSetId=tableConfig['dataSetId']
        ),
        dag=dag)


def getUpdateQueryTableMetadataTask(tableConfig, databaseConfig, dag):
    return UpdateHivePartitionOperator(
        task_id=f"{tableConfig['name']}__UpdateQueryTableMetadata",
        database=databaseConfig['hiveDatabase'],
        table=tableConfig['name'],
        metastore_conn_id='metastore_default',
        webhdfs_conn_id='webhdfs_default',
        dag=dag)


def getUpdateQueryTableDataTask(tableConfig, databaseConfig, dag, overridingConfig={}):
    suffix = '__UpdateQueryTableData'
    config = {
        'application': overridingConfig.get('sparkAppJar', getGlobalConfig()['sparkAppJar']),
        'conf': {
            'spark.sql.session.timeZone': 'UTC',
            'spark.driver.extraJavaOptions':
                '-Dpipeline.dataset.id=' + tableConfig.get('dataSetId') +
                ' -Dpipeline.hdfs.base.path=hdfs://nameservice1/user/datalake' +
                ' -Dpipeline.hive.database=' + databaseConfig.get('hiveDatabase') +
                ' -Dpipeline.hive.query.table=' + tableConfig.get('name') +
                ' -Dpipeline.kafka.topic=' + tableConfig.get('kafkaTopic') +
                ' -Dpipeline.columns.primary.keys=' + tableConfig.get('primaryKeys') +
                ' -Dpipeline.columns.update.order=' + tableConfig.get('updateOrderColumns') +
                ' -Dpipeline.column.partition.incr=partition' +
                ' -Dpipeline.column.partition.query=' + tableConfig.get('queryTablePartitionColumn') +
                ' -Dpipeline.preprocessings=' + json.dumps(json.dumps(tableConfig.get('preprocessings', [])))
        },
        'conn_id': tableConfig.get('sparkConnId'),
        'java_class': 'com.rakuten.dps.dataplatform.spark.job.UpdateQueryTableJob',
        'name': tableConfig.get('dataSetQn') + suffix,
        'pool': 'query_table_spark_job_pool',
    }
    if 'totalExecutorCores' in tableConfig:
        config['total_executor_cores'] = tableConfig.get('totalExecutorCores')
    if 'executorCores' in tableConfig:
        config['executor_cores'] = tableConfig.get('executorCores')
    if 'executorMemory' in tableConfig:
        config['executor_memory'] = tableConfig.get('executorMemory')
    return SparkSubmitOperator(task_id=tableConfig.get('name') + suffix, dag=dag, **config)


def getBulkLoadQueryTableTask(tableConfig, databaseConfig, dag, overridingConfig={}):
    suffix = '__BulkLoadQueryTable'
    config = {
        'application': overridingConfig.get('sparkAppJar', getGlobalConfig()['sparkAppJar']),
        'conf': {
            'spark.driver.extraJavaOptions':
                '-Dpipeline.dataset.id=' + tableConfig.get('dataSetId') +
                ' -Dpipeline.jdbc.url=' + tableConfig.get('jdbcUrl') +
                ' -Dpipeline.jdbc.driver=' + tableConfig.get('jdbcDriver') +
                ' -Dpipeline.jdbc.user.id=' + tableConfig.get('dbUserId') +
                ' -Dpipeline.jdbc.encrypted.password=' + tableConfig.get('dbEncryptedPassword') +
                ' -Dpipeline.hdfs.base.path=hdfs://nameservice1/user/datalake' +
                ' -Dpipeline.hive.database=' + databaseConfig.get('hiveDatabase') +
                ' -Dpipeline.hive.query.table=' + tableConfig.get('name') +
                ' -Dpipeline.column.partition.query=' + tableConfig.get('queryTablePartitionColumn') +
                ' -Dpipeline.columns.mysql.datetime.columns=' + tableConfig.get('datetimeColumns', '') +
                ' -Dpipeline.spark.date.add=' + get_bool_str_from_dict(tableConfig, 'dateColumnAddFlag') +
                ' -Dpipeline.mysql.server.timezone=' + tableConfig.get('serverTimezone', '')
        },
        'conn_id': tableConfig.get('sparkConnId'),
        'java_class': 'com.rakuten.dps.dataplatform.spark.job.BulkLoadQueryTableJob',
        'name': tableConfig.get('dataSetQn') + suffix,
        'pool': 'bulk_load_spark_job_pool',
    }
    if 'totalExecutorCores' in tableConfig:
        config['total_executor_cores'] = tableConfig.get('totalExecutorCores')
    if 'executorCores' in tableConfig:
        config['executor_cores'] = tableConfig.get('executorCores')
    if 'executorMemory' in tableConfig:
        config['executor_memory'] = tableConfig.get('executorMemory')
    return SparkSubmitOperator(task_id=tableConfig.get('name') + suffix, dag=dag, **config)


def getOracleHistMergeTask(tableConfig, databaseConfig, dag, overridingConfig={}):
    suffix = '__OracleHistMerge'
    return SparkSubmitOperator(
        task_id=tableConfig.get('name') + suffix,
        dag=dag,
        name=tableConfig.get('dataSetQn') + suffix,
        application=overridingConfig.get('sparkAppJar', getGlobalConfig()['sparkAppJar']),
        java_class='com.rakuten.dps.dataplatform.spark.job.OracleTableConsolidationJob',
        conn_id=tableConfig.get('sparkConnId'),
        application_args=[
            '--hdfsBasePath', 'hdfs://nameservice1/user/datalake',
            '--hiveDatabase', databaseConfig.get('hiveDatabase'),
            '--originalTableKafkaTopicName', tableConfig.get('kafkaTopic'),
            '--historicalTableKafkaTopicName', tableConfig.get('oracleHistTable').get('kafkaTopic'),
            '--timestampColumn', tableConfig.get('updateOrderColumns'),
            '--incrTablePartitionColumn', 'partition',
            '--retainHistIncrFiles', str(tableConfig.get('oracleHistTable').get('generateQueryTable')),
        ],
        pool='query_table_spark_job_pool',
        total_executor_cores=tableConfig.get('totalExecutorCores', 4),
        executor_cores=tableConfig.get('executorCores', 2),
        executor_memory=tableConfig.get('executorMemory', '12g'),
    )

def getDagsBasePath():
    return airflow_conf.get("core", "dags_folder")


def loadJsonConfig(subPath, configFile):
    if not os.path.isfile(os.path.join(getDagsBasePath(), subPath, configFile)):
        return None

    with open(os.path.join(getDagsBasePath(), subPath, configFile), 'r') as configFp:
        return json.loads(configFp.read())

def replace_date_pattern(pattern_string):
    dates = {'today' : '{{ ds_nodash }}',
             'today_dashed' : '{{ ds }}',
             'yesterday' : '{{ yesterday_ds_nodash }}',
             'yesterday_dashed' : '{{ yesterday_ds }}',
             'next_day': '{{ next_ds_nodash }}',
             'today_jst': "{{ (next_execution_date + macros.timedelta(hours=9)).strftime('%Y%m%d') }}"
             }
    return (pattern_string % dates)

def loadIniCfg(subPath, configFile):
    config = configparser.ConfigParser()
    if os.path.isfile(os.path.join(getDagsBasePath(), subPath, configFile)):
        config.read(os.path.join(getDagsBasePath(), subPath, configFile))
    return config


def send_slack_alert(msg_text):
    # SlackWebhookHook is buggy in v1.10.3, so using HttpHook
    HttpHook(http_conn_id='slack').run(
        endpoint=json.loads(BaseHook.get_connection('slack').get_extra())['webhook_token_pipeline_alerts'],
        data=json.dumps({'text': msg_text}),
        headers={'Content-type': 'application/json'},
        extra_options={'proxies': {'https': 'http://dev-proxy.db.rakuten.co.jp:9501'}}
    )

def getUpdateQueryTableDataTaskFile(tableConfig, databaseConfig, dag, overridingConfig={}):
    suffix = '__UpdateQueryTableDataFile'
    config = {
        'application': 'https://artifactory.rakuten-it.com/spdb-mvn-release/com/rakuten/dps/dataplatform/pipeline-spark-jobs_2.11/2.3_Paritosh/pipeline-spark-jobs_2.11-2.3_Paritosh-assembly.jar',
        'conf': {
            'spark.sql.session.timeZone': 'UTC'
        },
        'java_class': 'com.rakuten.dps.dataplatform.spark.job.ReadSpdbFileJob',
        'pool': 'query_table_spark_job_pool'
    }

    config['total_executor_cores'] = 3
    config['executor_memory'] = '4g'
    return SparkSubmitOperator(task_id=tableConfig.get('name') + suffix, dag=dag, **config)
